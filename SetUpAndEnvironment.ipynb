{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🔧 Session 1: Setup & Environment\n",
        "\n",
        "**Objective:**  \n",
        "Prepare the Colab environment for AI Agent development using LangChain, FAISS, PyPDF, and Gemini API (via Google Colab secrets).  \n",
        "\n",
        "**Why This Matters:**  \n",
        "- Everyone works in the same Colab environment.  \n",
        "- **Colab Secrets Manager** keeps API keys secure without uploading `.env` files.  \n",
        "- Ensures a smooth start before we dive into Retrieval-Augmented Generation (RAG).\n"
      ],
      "metadata": {
        "id": "fzKbytIgRfkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 1: Install Required Libraries\n",
        "We’ll install:\n",
        "- **LangChain** → Framework for building LLM-powered apps  \n",
        "- **FAISS** → Vector store for embeddings  \n",
        "- **PyPDF** → Load PDF files  \n",
        "- **python-dotenv** → Optional (not required if we use Colab secrets)  \n",
        "- **Google Generative AI SDK** → Access Gemini\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZQnh32XRxiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Ote6d_RT28",
        "outputId": "618d2fd5-a27d-4a35-9efa-93f4895442b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain faiss-cpu pypdf python-dotenv google-generativeai langchain_google_genai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 2: Setup Gemini API Key using Colab Secrets\n",
        "\n",
        "- In Colab:  \n",
        "  ` Secrets → Add new secret`  \n",
        "- Add a secret with key = **GEMINI_API_KEY**  \n",
        "- Then, we’ll fetch it in the notebook:\n"
      ],
      "metadata": {
        "id": "61DE3pwARe5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Load Gemini API key from Colab secrets\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    print(\"✅ Gemini API key retrieved from Colab Secrets!\")\n",
        "else:\n",
        "    print(\"❌ Gemini API key not found. Please add it in Colab Secrets.\")\n"
      ],
      "metadata": {
        "id": "6aE9ZHFPTeAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92d4e4b-3dd1-4cb7-ed2d-90e95c55077a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gemini API key retrieved from Colab Secrets!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 3: Verify Installation with Gemini Test Call\n",
        "We’ll run a simple Gemini call via LangChain to check setup.\n"
      ],
      "metadata": {
        "id": "7L_an7ZTiM6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Test prompt\n",
        "response = llm.invoke(\"Hello Gemini! Why is LangChain important in AI agent development?\")\n",
        "print(\"Gemini Response:\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0R9yFPjiK0-",
        "outputId": "4c9c8d09-ed1f-476f-e19f-16944f2d7b03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini Response: LangChain is incredibly important in AI agent development because it acts as a **powerful framework that bridges the gap between the raw intelligence of Large Language Models (LLMs) and the need for these models to interact with the real world, remember context, and perform complex, multi-step tasks.**\n",
            "\n",
            "Here's a breakdown of why it's so crucial:\n",
            "\n",
            "1.  **Empowering LLMs Beyond Text Generation:**\n",
            "    *   **The LLM Limitation:** LLMs are phenomenal at generating text, understanding context, and reasoning based on their training data. However, out-of-the-box, they are *just* text models. They can't browse the internet, query a database, run code, or remember past conversations beyond the current prompt window.\n",
            "    *   **LangChain's Solution:** LangChain provides the mechanisms to connect LLMs to these external capabilities. It turns a \"brain\" (the LLM) into an \"agent\" that can *act*.\n",
            "\n",
            "2.  **Orchestration and Chaining Complex Workflows:**\n",
            "    *   **The Problem:** Many real-world AI applications require more than a single LLM call. They involve a sequence of steps: retrieve data, analyze it, generate a response, perhaps perform an action, then summarize. Building these pipelines from scratch is complex and repetitive.\n",
            "    *   **LangChain's Solution:** Its core concept of \"chains\" allows developers to define sequences of operations (e.g., \"Take user input -> Pass to LLM -> Use LLM output to query a tool -> Get tool output -> Pass back to LLM for summarization -> Present final answer\"). This dramatically simplifies the creation of multi-step reasoning and action workflows.\n",
            "\n",
            "3.  **Enabling \"Agents\" with Tools:**\n",
            "    *   **The Agent Concept:** An AI agent, in this context, is an LLM that can *decide* which actions to take and which tools to use to achieve a goal.\n",
            "    *   **LangChain's Solution:** It provides the framework for defining \"tools\" (e.g., a search engine API, a calculator, a custom database query, a code interpreter) and then allows an LLM to dynamically select and use these tools based on the user's request. This is perhaps its most transformative feature for agent development, allowing LLMs to overcome their inherent limitations and interact with external systems.\n",
            "\n",
            "4.  **Memory Management for Stateful Interactions:**\n",
            "    *   **The Problem:** LLMs are stateless by default; they don't remember previous turns in a conversation unless you explicitly pass the entire history in each prompt, which quickly becomes expensive and hits token limits.\n",
            "    *   **LangChain's Solution:** It offers various \"memory\" modules that allow agents to remember past interactions, summarize conversations, or retrieve specific pieces of information from their history, making conversational AI agents far more natural and effective.\n",
            "\n",
            "5.  **Retrieval Augmented Generation (RAG):**\n",
            "    *   **The Problem:** LLMs can \"hallucinate\" or provide outdated information because their knowledge is limited to their training data.\n",
            "    *   **LangChain's Solution:** It provides robust components for RAG, allowing agents to fetch relevant, up-to-date information from external knowledge bases (documents, databases, websites) *before* generating a response. This grounds the LLM's answers in factual data, significantly improving accuracy and trustworthiness.\n",
            "\n",
            "6.  **Modularity and Extensibility:**\n",
            "    *   **Developer Flexibility:** LangChain is designed to be highly modular. You can easily swap out different LLM providers (OpenAI, Anthropic, Hugging Face models), different tools, different memory types, and different chain types.\n",
            "    *   **Customization:** This allows developers to build highly customized agents tailored to specific use cases without reinventing the wheel for every component.\n",
            "\n",
            "7.  **Rapid Prototyping and Development:**\n",
            "    *   **Accelerated Innovation:** By abstracting away much of the complexity of integrating LLMs with other systems, LangChain significantly speeds up the development cycle for new AI agent applications. Developers can focus on the logic and user experience rather than low-level plumbing.\n",
            "\n",
            "In essence, LangChain transforms LLMs from powerful text generators into **intelligent, adaptable, and interactive agents** that can understand, reason, act, and learn within complex environments. It's the operating system for building the next generation of AI applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "indRz-fw1RVW"
      }
    }
  ]
}