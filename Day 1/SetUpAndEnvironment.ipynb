{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üîß Session 1: Setup & Environment\n",
        "\n",
        "**Objective:**  \n",
        "Prepare the Colab environment for AI Agent development using LangChain, FAISS, PyPDF, and Gemini API (via Google Colab secrets).  \n",
        "\n",
        "**Why This Matters:**  \n",
        "- Everyone works in the same Colab environment.  \n",
        "- **Colab Secrets Manager** keeps API keys secure without uploading `.env` files.  \n",
        "- Ensures a smooth start before we dive into Retrieval-Augmented Generation (RAG).\n"
      ],
      "metadata": {
        "id": "fzKbytIgRfkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Step 1: Install Required Libraries\n",
        "We‚Äôll install:\n",
        "- **LangChain** ‚Üí Framework for building LLM-powered apps  \n",
        "- **FAISS** ‚Üí Vector store for embeddings  \n",
        "- **PyPDF** ‚Üí Load PDF files  \n",
        "- **python-dotenv** ‚Üí Optional (not required if we use Colab secrets)  \n",
        "- **Google Generative AI SDK** ‚Üí Access Gemini\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZQnh32XRxiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Ote6d_RT28",
        "outputId": "48bf97e3-87b2-4bbc-ad75-15d9f937aa6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.3.27 faiss-cpu==1.12.0 pypdf==6.1.2 python-dotenv==1.1.1 google-generativeai==0.8.5 langchain_google_genai==2.0.10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain faiss-cpu pypdf python-dotenv google-generativeai langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-i5bnqKYR7D",
        "outputId": "57a42dc2-57ef-4d6b-df45-a3c5c098e0a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.27\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: \n",
            "---\n",
            "Name: faiss-cpu\n",
            "Version: 1.12.0\n",
            "Summary: A library for efficient similarity search and clustering of dense vectors.\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Kota Yamaguchi <yamaguchi_kota@cyberagent.co.jp>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: numpy, packaging\n",
            "Required-by: \n",
            "---\n",
            "Name: pypdf\n",
            "Version: 6.1.2\n",
            "Summary: A pure-python PDF library capable of splitting, merging, cropping, and transforming PDF files\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Mathieu Fenniak <biziqe@mathieu.fenniak.net>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "---\n",
            "Name: python-dotenv\n",
            "Version: 1.1.1\n",
            "Summary: Read key-value pairs from a .env file and set them as environment variables\n",
            "Home-page: https://github.com/theskumar/python-dotenv\n",
            "Author: Saurabh Kumar\n",
            "Author-email: me+github@saurabh-kumar.com\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: \n",
            "Required-by: google-adk, pydantic-settings\n",
            "---\n",
            "Name: google-generativeai\n",
            "Version: 0.8.5\n",
            "Summary: Google Generative AI High level API client library and tools.\n",
            "Home-page: https://github.com/google/generative-ai-python\n",
            "Author: Google LLC\n",
            "Author-email: googleapis-packages@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: google-ai-generativelanguage, google-api-core, google-api-python-client, google-auth, protobuf, pydantic, tqdm, typing-extensions\n",
            "Required-by: langchain-google-genai\n",
            "---\n",
            "Name: langchain-google-genai\n",
            "Version: 2.0.10\n",
            "Summary: An integration package connecting Google's genai package and LangChain\n",
            "Home-page: https://github.com/langchain-ai/langchain-google\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: filetype, google-generativeai, langchain-core, pydantic\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Step 2: Setup Gemini API Key using Colab Secrets\n",
        "\n",
        "- In Colab:  \n",
        "  ` Secrets ‚Üí Add new secret`  \n",
        "- Add a secret with key = **GEMINI_API_KEY**  \n",
        "- Then, we‚Äôll fetch it in the notebook:\n"
      ],
      "metadata": {
        "id": "61DE3pwARe5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Load Gemini API key from Colab secrets\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    print(\"‚úÖ Gemini API key retrieved from Colab Secrets!\")\n",
        "else:\n",
        "    print(\"‚ùå Gemini API key not found. Please add it in Colab Secrets.\")\n"
      ],
      "metadata": {
        "id": "6aE9ZHFPTeAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f764854-a917-4358-83b2-98e8328dce19"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API key retrieved from Colab Secrets!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Step 3: Verify Installation with Gemini Test Call\n",
        "We‚Äôll run a simple Gemini call via LangChain to check setup.\n"
      ],
      "metadata": {
        "id": "7L_an7ZTiM6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Test prompt\n",
        "response = llm.invoke(\"Hello Gemini! Why is LangChain important in AI agent development?\")\n",
        "print(\"Gemini Response:\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0R9yFPjiK0-",
        "outputId": "884c1d1d-ce19-4e60-b97c-0b5daa810338"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini Response: LangChain is incredibly important in AI agent development because it provides a **structured, modular, and extensible framework** for building applications that leverage Large Language Models (LLMs) to go beyond simple text generation.\n",
            "\n",
            "Here's a breakdown of why it's so crucial:\n",
            "\n",
            "1.  **Bridging the Gap Between LLMs and the Real World:**\n",
            "    *   **LLMs are powerful but stateless and isolated.** They excel at understanding and generating text, but they don't inherently have memory, access to real-time information, or the ability to perform actions.\n",
            "    *   **LangChain acts as an orchestration layer.** It enables LLMs to interact with external data sources, APIs, and other tools, effectively giving them \"eyes, ears, and hands\" to operate in the real world.\n",
            "\n",
            "2.  **Enabling \"Agents\" with Reasoning and Action:**\n",
            "    *   This is perhaps LangChain's most significant contribution. It allows you to define **AI Agents** where the LLM becomes the \"brain\" that decides what to do next.\n",
            "    *   **Tools:** LangChain provides a way for LLMs to use external tools (e.g., search engines, calculators, APIs, databases). The LLM reasons about the user's request, decides which tool to use, executes it, and incorporates the results back into its thought process. This is fundamental for building dynamic, problem-solving agents.\n",
            "    *   **Reasoning Loop:** LangChain facilitates a multi-step reasoning process where the LLM can observe, think, plan, and act, iteratively improving its response or achieving a goal.\n",
            "\n",
            "3.  **Managing Context and Memory:**\n",
            "    *   **Stateless LLMs:** By default, each interaction with an LLM is independent. For a conversational agent, this is a major limitation.\n",
            "    *   **Memory Modules:** LangChain offers various memory implementations (e.g., conversational buffer, summarization memory, entity memory) that allow agents to remember past interactions, context, and specific information throughout a conversation. This is vital for coherent and personalized long-running interactions.\n",
            "\n",
            "4.  **Chaining Complex Operations:**\n",
            "    *   Many real-world tasks require more than a single LLM call. LangChain introduces the concept of **Chains**, which are sequences of operations.\n",
            "    *   You can chain together LLM calls, data processing steps, prompt formatting, and tool invocations. This allows you to break down complex problems into smaller, manageable steps, making the development of sophisticated workflows much easier.\n",
            "    *   **Examples:** A chain could first retrieve relevant documents (RAG), then summarize them, then use the summary to answer a user's question.\n",
            "\n",
            "5.  **Retrieval Augmented Generation (RAG):**\n",
            "    *   **Grounding LLMs in Specific Data:** LLMs have a knowledge cutoff and can sometimes \"hallucinate.\" LangChain makes it easy to integrate with external knowledge bases (vector databases, document loaders).\n",
            "    *   **Better Accuracy and Relevance:** By retrieving relevant information from your own data sources and feeding it to the LLM as context, LangChain enables more accurate, up-to-date, and domain-specific responses. This is critical for enterprise applications.\n",
            "\n",
            "6.  **Prompt Engineering and Management:**\n",
            "    *   **Prompt Templates:** LangChain offers structured ways to create and manage prompt templates, making it easier to inject dynamic variables and ensure consistency across different LLM calls.\n",
            "    *   **Output Parsers:** It helps parse and structure the output from LLMs, converting raw text into usable data formats (e.g., JSON, Pydantic models).\n",
            "\n",
            "7.  **Modularity and Extensibility:**\n",
            "    *   LangChain is designed with modularity in mind. You can easily swap out different LLM providers, memory types, tools, and vector stores without rewriting significant portions of your application.\n",
            "    *   This flexibility allows developers to experiment, iterate quickly, and adapt their agents to new requirements or technologies.\n",
            "\n",
            "8.  **Accelerated Development:**\n",
            "    *   By providing pre-built components and abstractions for common LLM patterns, LangChain significantly reduces the boilerplate code and complexity involved in building advanced AI applications. This allows developers to focus on the unique logic of their agents rather than the plumbing.\n",
            "\n",
            "In essence, LangChain transforms LLMs from powerful text generators into intelligent, interactive, and context-aware agents that can perform complex tasks, leverage external information, and engage in meaningful, multi-turn conversations. It's an indispensable tool for anyone looking to build sophisticated AI applications with LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "indRz-fw1RVW"
      }
    }
  ]
}