{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iePczeLinW4M"
      },
      "source": [
        "# ğŸ§‘â€ğŸ’» Session 6: Retrieval Augmented Generation (RAG)\n",
        "\n",
        "RAG combines:\n",
        "1. **Retriever** â†’ Fetches relevant documents from a vector store.  \n",
        "2. **LLM** â†’ Generates context-aware responses using query + retrieved docs.  \n",
        "\n",
        "In this session, we will:\n",
        "- Store documents with **Gemini Embeddings**  \n",
        "- Retrieve docs from **Chroma**  \n",
        "- Use **Groq LLM** for answering questions  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih0sehbCnPTj",
        "outputId": "708e758d-e774-4348-deff-a00c975ad8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.7/20.7 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.16.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.16.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# ğŸ“Œ Install dependencies\n",
        "!pip install -q langchain==0.3.27 langchain-community==0.3.31 langchain-groq==0.3.8 langchain-google-genai==2.1.12 chromadb==1.2.1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain langchain-community langchain-groq langchain-google-genai chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXZHf7bioLTi",
        "outputId": "8bd841a9-0a67-49d2-8890-c1f4d25fd84f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.27\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: langchain-community\n",
            "---\n",
            "Name: langchain-community\n",
            "Version: 0.3.31\n",
            "Summary: Community contributed LangChain integrations.\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: aiohttp, dataclasses-json, httpx-sse, langchain, langchain-core, langsmith, numpy, pydantic-settings, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-groq\n",
            "Version: 0.3.8\n",
            "Summary: An integration package connecting Groq and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: groq, langchain-core\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-google-genai\n",
            "Version: 2.1.12\n",
            "Summary: An integration package connecting Google's genai package and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: filetype, google-ai-generativelanguage, langchain-core, pydantic\n",
            "Required-by: \n",
            "---\n",
            "Name: chromadb\n",
            "Version: 1.2.1\n",
            "Summary: Chroma.\n",
            "Home-page: https://github.com/chroma-core/chroma\n",
            "Author: \n",
            "Author-email: Jeff Huber <jeff@trychroma.com>, Anton Troynikov <anton@trychroma.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: bcrypt, build, grpcio, httpx, importlib-resources, jsonschema, kubernetes, mmh3, numpy, onnxruntime, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-sdk, orjson, overrides, posthog, pybase64, pydantic, pypika, pyyaml, rich, tenacity, tokenizers, tqdm, typer, typing-extensions, uvicorn\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y1d1tfNno4j"
      },
      "source": [
        "## ğŸ”‘ Setup API Keys\n",
        "- Google Gemini API key â†’ for embeddings  \n",
        "- Groq API key â†’ for LLM (LLaMA models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L0cNScAVpEb_"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byAbKgqRnzVD"
      },
      "source": [
        "## ğŸ“„ Step 1: Create Documents\n",
        "Weâ€™ll use IPL players knowledge base.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3ciDLFiynuZD"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "docs = [\n",
        "    Document(\n",
        "        page_content=\"Virat Kohli is one of the most successful batsmen in IPL history and has captained RCB.\",\n",
        "        metadata={\"team\": \"Royal Challengers Bangalore\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Rohit Sharma is the most successful captain in IPL history, winning five titles with Mumbai Indians.\",\n",
        "        metadata={\"team\": \"Mumbai Indians\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"MS Dhoni has led Chennai Super Kings to multiple IPL titles and is known as Captain Cool.\",\n",
        "        metadata={\"team\": \"Chennai Super Kings\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Jasprit Bumrah is a leading fast bowler for Mumbai Indians, famous for his yorkers.\",\n",
        "        metadata={\"team\": \"Mumbai Indians\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Ravindra Jadeja is an all-rounder for Chennai Super Kings, contributing with bat, ball, and fielding.\",\n",
        "        metadata={\"team\": \"Chennai Super Kings\"}\n",
        "    )\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huvDaYHIn7dy"
      },
      "source": [
        "## ğŸ—‚ï¸ Step 2: Store Documents with Gemini Embeddings in Chroma\n",
        "Weâ€™ll use **GoogleGenerativeAIEmbeddings** for vector representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsutk8GKn_GP",
        "outputId": "9a4ce3fc-220a-4556-b6b8-9c1930319d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3914003044.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vector_store = Chroma(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f8e28e1f-7b35-4435-aabe-828e5e2f58ea',\n",
              " 'b509e1ea-8713-442d-a78c-75339ed3346b',\n",
              " 'aea8106e-4f87-402e-a32c-0db2e83de00a',\n",
              " 'a37ca89e-527c-49d2-9f4e-c9817e6ddcae',\n",
              " 'bcd29172-fa1b-455d-b76c-d004dec348c9']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\", google_api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Create Chroma vector store\n",
        "vector_store = Chroma(\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"rag_chroma_db\",\n",
        "    collection_name=\"ipl_docs\"\n",
        ")\n",
        "\n",
        "# Add documents\n",
        "vector_store.add_documents(docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdT_0TR8pa6W"
      },
      "source": [
        "## ğŸ” Step 3: Create Retriever\n",
        "Retriever fetches relevant chunks from Chroma.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8ud9N2tZpdnM"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsB0mG1kpa0L"
      },
      "source": [
        "## ğŸ§  Step 4: Initialize Groq LLM\n",
        "We use Groq-hosted LLaMA 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Gg2x1hT4pjv5"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Load API key\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "llm = ChatGroq(\n",
        "    model=\"openai/gpt-oss-20b\",\n",
        "    api_key=GROQ_API_KEY,\n",
        "    temperature=0.3,\n",
        "    max_tokens=200\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq6m7AZRp9UW"
      },
      "source": [
        "## ğŸ”— Step 5: Create RAG Chain\n",
        "Combine retriever + LLM into a RetrievalQA pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KaA7J2zJp8J4"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-PN9IKQqBn9"
      },
      "source": [
        "## ğŸ’¬ Step 6: Ask Questions\n",
        "Test RAG pipeline with cricket-related queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd4c8-K5qCm3",
        "outputId": "238c49d9-6b32-414b-bf3b-2a15e9380904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Who is the most successful IPL captain?\n",
            "Answer: The most successful IPL captain is **Rohit Sharma**, who has led Mumbai Indians to five championship titles.\n",
            "\n",
            "Sources: [{'team': 'Mumbai Indians'}, {'team': 'Royal Challengers Bangalore'}]\n",
            "\n",
            "Query: Which bowler is famous for yorkers?\n",
            "Answer: The bowler famous for his yorkers is **Jaspritâ€¯Bumrah**.\n",
            "\n",
            "Sources: [{'team': 'Mumbai Indians'}, {'team': 'Mumbai Indians'}]\n"
          ]
        }
      ],
      "source": [
        "# Query 1\n",
        "query = \"Who is the most successful IPL captain?\"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Answer:\", response[\"result\"])\n",
        "print(\"\\nSources:\", [doc.metadata for doc in response[\"source_documents\"]])\n",
        "\n",
        "# Query 2\n",
        "query2 = \"Which bowler is famous for yorkers?\"\n",
        "response2 = qa_chain.invoke({\"query\": query2})\n",
        "\n",
        "print(\"\\nQuery:\", query2)\n",
        "print(\"Answer:\", response2[\"result\"])\n",
        "print(\"\\nSources:\", [doc.metadata for doc in response2[\"source_documents\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkVJfECpph4H"
      },
      "source": [
        "# âœ… Summary\n",
        "- Used **Google Gemini Embeddings** to vectorize documents  \n",
        "- Stored + Retrieved docs from **Chroma**  \n",
        "- Connected retriever with **Groq LLM**  \n",
        "- Answered queries using **RAG pipeline**  \n",
        "\n",
        "ğŸ‘‰ Next session: **Advanced RAG â€“ Custom Prompts, Comparisons, and Deep Dive**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}