{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iePczeLinW4M"
      },
      "source": [
        "# 🧑‍💻 Session 6: Retrieval Augmented Generation (RAG)\n",
        "\n",
        "RAG combines:\n",
        "1. **Retriever** → Fetches relevant documents from a vector store.  \n",
        "2. **LLM** → Generates context-aware responses using query + retrieved docs.  \n",
        "\n",
        "In this session, we will:\n",
        "- Store documents with **Gemini Embeddings**  \n",
        "- Retrieve docs from **Chroma**  \n",
        "- Use **Groq LLM** for answering questions  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih0sehbCnPTj",
        "outputId": "34f63583-4537-49af-8ae0-cd8b58197d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 📌 Install dependencies\n",
        "!pip install -qU langchain langchain-community langchain-groq langchain-google-genai chromadb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y1d1tfNno4j"
      },
      "source": [
        "## 🔑 Setup API Keys\n",
        "- Google Gemini API key → for embeddings  \n",
        "- Groq API key → for LLM (LLaMA models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0cNScAVpEb_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"NO_GCE\"] = \"true\"  # Add this to disable metadata service checks in Colab\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"[YOUR_GEMINI_API_KEY]\"  # Replace with your actual Google API key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byAbKgqRnzVD"
      },
      "source": [
        "## 📄 Step 1: Create Documents\n",
        "We’ll use IPL players knowledge base.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3ciDLFiynuZD"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "docs = [\n",
        "    Document(\n",
        "        page_content=\"Virat Kohli is one of the most successful batsmen in IPL history and has captained RCB.\",\n",
        "        metadata={\"team\": \"Royal Challengers Bangalore\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Rohit Sharma is the most successful captain in IPL history, winning five titles with Mumbai Indians.\",\n",
        "        metadata={\"team\": \"Mumbai Indians\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"MS Dhoni has led Chennai Super Kings to multiple IPL titles and is known as Captain Cool.\",\n",
        "        metadata={\"team\": \"Chennai Super Kings\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Jasprit Bumrah is a leading fast bowler for Mumbai Indians, famous for his yorkers.\",\n",
        "        metadata={\"team\": \"Mumbai Indians\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Ravindra Jadeja is an all-rounder for Chennai Super Kings, contributing with bat, ball, and fielding.\",\n",
        "        metadata={\"team\": \"Chennai Super Kings\"}\n",
        "    )\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huvDaYHIn7dy"
      },
      "source": [
        "## 🗂️ Step 2: Store Documents with Gemini Embeddings in Chroma\n",
        "We’ll use **GoogleGenerativeAIEmbeddings** for vector representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsutk8GKn_GP",
        "outputId": "01f13c13-85d0-45fd-da2e-aac97454c44c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0a1713af-ce1f-4bcf-96fe-3e4f0da8d891',\n",
              " 'bd643bd2-78e6-4e1e-b318-c32747104443',\n",
              " 'f9a9932b-61d1-4f99-9155-53d616189d0e',\n",
              " 'e0676031-f087-4b2c-a8dc-8c5ea23409fb',\n",
              " 'b545d6c3-6e49-43a5-97a9-7a0ec92d2c68']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
        "\n",
        "# Create Chroma vector store\n",
        "vector_store = Chroma(\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"rag_chroma_db\",\n",
        "    collection_name=\"ipl_docs\"\n",
        ")\n",
        "\n",
        "# Add documents\n",
        "vector_store.add_documents(docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdT_0TR8pa6W"
      },
      "source": [
        "## 🔎 Step 3: Create Retriever\n",
        "Retriever fetches relevant chunks from Chroma.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8ud9N2tZpdnM"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsB0mG1kpa0L"
      },
      "source": [
        "## 🧠 Step 4: Initialize Groq LLM\n",
        "We use Groq-hosted LLaMA 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Gg2x1hT4pjv5"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Load API key\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "llm = ChatGroq(\n",
        "    model=\"openai/gpt-oss-20b\",\n",
        "    api_key=GROQ_API_KEY,\n",
        "    temperature=0.3,\n",
        "    max_tokens=200\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq6m7AZRp9UW"
      },
      "source": [
        "## 🔗 Step 5: Create RAG Chain\n",
        "Combine retriever + LLM into a RetrievalQA pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KaA7J2zJp8J4"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-PN9IKQqBn9"
      },
      "source": [
        "## 💬 Step 6: Ask Questions\n",
        "Test RAG pipeline with cricket-related queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd4c8-K5qCm3",
        "outputId": "c6403802-5867-4d23-cb6a-53c5da0a6059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Who is the most successful IPL captain?\n",
            "Answer: The most successful IPL captain is **Rohit Sharma**, who has led Mumbai Indians to five titles.\n",
            "\n",
            "Sources: [{'team': 'Mumbai Indians'}, {'team': 'Royal Challengers Bangalore'}]\n",
            "\n",
            "Query: Which bowler is famous for yorkers?\n",
            "Answer: The bowler famous for his yorkers is **Jasprit Bumrah**.\n",
            "\n",
            "Sources: [{'team': 'Mumbai Indians'}, {'team': 'Mumbai Indians'}]\n"
          ]
        }
      ],
      "source": [
        "# Query 1\n",
        "query = \"Who is the most successful IPL captain?\"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Answer:\", response[\"result\"])\n",
        "print(\"\\nSources:\", [doc.metadata for doc in response[\"source_documents\"]])\n",
        "\n",
        "# Query 2\n",
        "query2 = \"Which bowler is famous for yorkers?\"\n",
        "response2 = qa_chain.invoke({\"query\": query2})\n",
        "\n",
        "print(\"\\nQuery:\", query2)\n",
        "print(\"Answer:\", response2[\"result\"])\n",
        "print(\"\\nSources:\", [doc.metadata for doc in response2[\"source_documents\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkVJfECpph4H"
      },
      "source": [
        "# ✅ Summary\n",
        "- Used **Google Gemini Embeddings** to vectorize documents  \n",
        "- Stored + Retrieved docs from **Chroma**  \n",
        "- Connected retriever with **Groq LLM**  \n",
        "- Answered queries using **RAG pipeline**  \n",
        "\n",
        "👉 Next session: **Advanced RAG – Custom Prompts, Comparisons, and Deep Dive**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
