{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🤖 Session 2: Fetching LLM Responses (Groq + LangChain v3)\n",
        "\n",
        "**Objective:**  \n",
        "Learn how to call **Groq-hosted LLMs** via LangChain, run test prompts, and explore parameters.  \n",
        "\n",
        "**Why This Matters:**  \n",
        "- Groq provides ultra-low latency responses with models like **LLaMA 3** and **Mixtral**.  \n",
        "- Before diving into RAG, participants need to practice **direct LLM calls**.  \n"
      ],
      "metadata": {
        "id": "luJYm8zA71Oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 1: Install Required Libraries\n",
        "We’ll install:\n",
        "- **LangChain v3**  \n",
        "- **LangChain Groq integration**  \n"
      ],
      "metadata": {
        "id": "_1zDC5OP8CP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain==0.3.27 langchain-groq==0.3.8\n"
      ],
      "metadata": {
        "id": "HZLyUF1Z74Zn",
        "outputId": "d134b126-adef-46b1-e28c-d0ac0d94f7bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m133.1/135.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1xAuAe7addo",
        "outputId": "ed3aea20-24f4-4f9b-d64f-75c8fe00e0a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.27\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-groq\n",
            "Version: 0.3.8\n",
            "Summary: An integration package connecting Groq and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: groq, langchain-core\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 2: Setup Groq API Key\n",
        "We’ll fetch it securely from **Colab Secrets**.  \n",
        "\n",
        "In Colab:  \n",
        "- Go to ` Secrets → Add new secret`  \n",
        "- Key = `GROQ_API_KEY`  \n",
        "- Value = your Groq key from [console.groq.com](https://console.groq.com/keys)\n"
      ],
      "metadata": {
        "id": "PapMQNZt8IWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Load Groq API key\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "if GROQ_API_KEY:\n",
        "    print(\"✅ Groq API key retrieved!\")\n",
        "else:\n",
        "    print(\"❌ Please add GROQ_API_KEY in Colab Secrets.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s4C70Yk8Ngt",
        "outputId": "da8a115e-ccd7-4584-be06-b5388d6ae402"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Groq API key retrieved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 3: Initialize Groq LLM\n",
        "We’ll use the **ChatGroq** wrapper from LangChain.\n"
      ],
      "metadata": {
        "id": "aKPC8lKi8RT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Initialize with LLaMA-3 (you can also use mixtral-8x7b)\n",
        "llm = ChatGroq(\n",
        "    model=\"openai/gpt-oss-20b\",\n",
        "    api_key=GROQ_API_KEY,\n",
        "    temperature=0.7,\n",
        "    max_tokens=256\n",
        ")\n"
      ],
      "metadata": {
        "id": "JKrlQNWk8SN7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 4: Run Basic Prompts\n",
        "Test Q&A and summarization with Groq.\n"
      ],
      "metadata": {
        "id": "3W1bHvjT8mcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Q&A\n",
        "response = llm.invoke(\"What is Retrieval-Augmented Generation (RAG) in AI?\")\n",
        "print(response)\n",
        "# print(\"Groq Response 1:\\n\", response.content)\n",
        "\n",
        "# Summarization\n",
        "text = \"\"\"LangChain is a framework for building AI-powered applications.\n",
        "It provides tools for prompt management, chaining, agents, memory,\n",
        "and integrations with external APIs and vector databases.\"\"\"\n",
        "summary = llm.invoke(f\"Summarize this in 2 sentences:\\n\\n{text}\")\n",
        "print(\"\\nSummary:\\n\", summary.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hZrrrb68nTb",
        "outputId": "28c175eb-97af-4b69-875b-236cf48f8b48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='## Retrieval‑Augmented Generation (RAG) – A Quick‑Start Guide\\n\\n> **RAG** is a hybrid AI architecture that *retrieves* relevant text from an external knowledge source and *generates* a response conditioned on that retrieved content.  \\n> It blends the strengths of **retrieval‑based systems** (accuracy, grounding) with **generative models** (fluency, flexibility).\\n\\n---\\n\\n### 1. The Core Idea\\n\\n| Step | What Happens | Why It Matters |\\n|------|--------------|----------------|\\n| **1.' additional_kwargs={'reasoning_content': 'We need to explain Retrieval-Augmented Generation. Provide overview, components, typical architecture, benefits, examples, applications. Maybe mention RAG as introduced by Facebook AI. Explain retrieval step, knowledge base, encoder, decoder, fusion. Discuss variations: dense retrieval, sparse retrieval, hybrid. Mention use in open-domain QA, chatbots, summarization, etc. Provide potential pitfalls: retrieval errors, hallucinations, latency. Provide examples: RAG-Token, RAG-Sequence. Provide references. Also mention difference from pure generative models. Provide practical usage: training, fine-tuning, vector stores. Provide code snippet? Might be too detailed. Provide summary.'} response_metadata={'token_usage': {'completion_tokens': 256, 'prompt_tokens': 84, 'total_tokens': 340, 'completion_time': 0.29383224, 'prompt_time': 0.012337885, 'queue_time': 0.326153532, 'total_time': 0.306170125}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_4f7e7dc26e', 'service_tier': 'on_demand', 'finish_reason': 'length', 'logprobs': None} id='run--8e5e070a-e3ee-48d6-b71a-86b605ed0538-0' usage_metadata={'input_tokens': 84, 'output_tokens': 256, 'total_tokens': 340}\n",
            "\n",
            "Summary:\n",
            " LangChain is a framework that empowers developers to build AI‑powered applications by offering tools for prompt management, chaining, agents, and memory. It also integrates seamlessly with external APIs and vector databases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 5: Exploring Parameters\n",
        "- `temperature` → Randomness of output (0 = factual, 1 = creative).  \n",
        "- `max_tokens` → Maximum response length.  \n"
      ],
      "metadata": {
        "id": "BLSJMm779EaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Low temperature (factual)\n",
        "cold = ChatGroq(model=\"openai/gpt-oss-20b\", api_key=GROQ_API_KEY, temperature=0.0)\n",
        "print(\"Cold Response:\", cold.invoke(\"Suggest one creative use of AI in education\").content)\n",
        "print(\"_\"*50)\n",
        "\n",
        "# High temperature (creative)\n",
        "hot = ChatGroq(model=\"openai/gpt-oss-20b\", api_key=GROQ_API_KEY, temperature=0.9)\n",
        "print(\"\\nHot Response:\", hot.invoke(\"Suggest one creative use of AI in education\").content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNIcUKMO9GDu",
        "outputId": "d0334435-b093-4dbc-b5fa-b005693752b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cold Response: **AI‑Powered “Learning Lab” – A Dynamic, Adaptive, Collaborative Sandbox**\n",
            "\n",
            "**Concept**  \n",
            "Create a virtual “Learning Lab” that blends AI, mixed‑reality, and real‑world data to let students design, test, and iterate on projects that span multiple subjects. Think of it as a digital science‑fair, coding bootcamp, and design sprint rolled into one, but powered by AI to personalize, scaffold, and amplify creativity.\n",
            "\n",
            "**How it Works**\n",
            "\n",
            "| Feature | AI Role | Student Experience |\n",
            "|---------|---------|--------------------|\n",
            "| **Project Ideation** | Natural‑language generation + knowledge graph | Students type a problem statement (“How can we reduce plastic waste in our school?”) and the AI suggests interdisciplinary angles (biology, economics, design, coding). |\n",
            "| **Skill Gap Analysis** | Adaptive assessment + skill mapping | The AI evaluates the student’s current knowledge and recommends micro‑learning modules (videos, quizzes, interactive simulations) to fill gaps. |\n",
            "| **Collaborative Design** | Real‑time language translation + sentiment analysis | Teams from different language backgrounds can co‑create prototypes; the AI moderates communication, highlights misunderstandings, and suggests clarifications. |\n",
            "| **Simulation & Testing** | Physics engine + predictive modeling | Students build virtual prototypes (e.g., a biodegradable packaging design) and run simulations that predict performance, cost, and environmental impact. |\n",
            "| **Feedback Loop** | Generative critique + emotional tone detection | After each iteration, the AI provides constructive feedback, celebrates progress, and nudges students toward deeper inquiry. |\n",
            "| **Real‑World Connection** | Data mining + API integration | The AI pulls in current news, research papers, and local statistics, allowing students to anchor their projects in real‑time data. |\n",
            "| **Showcase & Reflection** | Automated video editing + reflective prompts | The AI compiles a polished presentation video and asks reflective questions (“What surprised you?” “What would you do differently?”). |\n",
            "\n",
            "**Why It’s Creative**\n",
            "\n",
            "1. **Cross‑Disciplinary Fusion** – Students naturally weave science, math, art, and humanities into one project, mirroring real‑world problem solving.\n",
            "2. **Personalized, Adaptive Learning** – AI tailors the learning path to each student’s strengths, interests, and pace, turning the classroom into a “learning ecosystem.”\n",
            "3. **Immediate, Data‑Driven Feedback** – Instead of waiting for a teacher’s review, students receive instant, actionable insights, keeping momentum high.\n",
            "4. **Global Collaboration** – Language translation and sentiment analysis break down barriers, letting students partner with peers worldwide.\n",
            "5. **Real‑World Impact** – By integrating live data and local context, projects feel relevant and can even influence community decisions (e.g., a school’s waste‑management plan).\n",
            "\n",
            "**Implementation Snapshot**\n",
            "\n",
            "- **Phase 1 (Pilot)** – 1‑2 classes use the platform for a single interdisciplinary project (e.g., designing a low‑cost water purifier).  \n",
            "- **Phase 2 (Scale)** – Expand to multiple subjects, integrate with existing LMS, and add teacher dashboards for monitoring progress.  \n",
            "- **Phase 3 (Community)** – Open the sandbox to external partners (NGOs, industry mentors) for real‑world feedback and potential funding.\n",
            "\n",
            "**Outcome**  \n",
            "Students leave the Learning Lab not only with deeper knowledge but also with a portfolio of AI‑enhanced projects, collaborative skills, and a mindset that sees problems as opportunities for creative, data‑driven solutions.\n",
            "__________________________________________________\n",
            "\n",
            "Hot Response: **AI‑Powered Adaptive History Museum**\n",
            "\n",
            "Create a virtual museum that AI “curates” on‑demand for each student.  \n",
            "- **Dynamic Exhibit Selection** – As a student explores a time period, the AI tracks the topics they click on, ask questions about, or spend extra time with and then pulls in the most relevant artifacts, primary documents, or multimedia clips from a huge database.  \n",
            "- **Real‑time Guided Narrative** – Instead of a static tour guide, the AI speaks in natural language, tailoring explanations, analogies, and depth of detail to the student’s reading level, prior knowledge, and expressed interests.  \n",
            "- **Interactive Quizzes & Debates** – After each “room,” the AI generates personalized quiz questions, prompts for critical thinking, or even simulates a debate with a historical figure.  \n",
            "- **Collaborative Exploration** – Multiple students can join the same virtual walk, and the AI mediates group discussions, ensuring each voice is heard and connecting their ideas to the broader narrative.  \n",
            "\n",
            "This use turns passive textbook learning into a living, responsive experience that adapts to every student’s curiosity, making complex historical contexts engaging and personalized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Exercise\n",
        "1. Change the model compare answers.  \n",
        "2. Try summarizing a longer passage of your choice.  \n",
        "3. Ask Groq to **generate pros & cons** of using RAG vs plain LLMs.  \n"
      ],
      "metadata": {
        "id": "e2_bacxD9p6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎯 Summary\n",
        "- Installed **LangChain v3** with Groq integration.  \n",
        "- Made simple LLM calls with `ChatGroq`.  \n",
        "- Learned how to adjust parameters (`temperature`, `max_tokens`).  \n",
        "\n",
        "**Next Notebook → Document Loaders in LangChain**  \n"
      ],
      "metadata": {
        "id": "Ssv8lLii9uI2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}